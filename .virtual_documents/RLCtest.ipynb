import gym
print(gym.__version__)


import gym
env = gym.make('CartPole-v0')
env.reset()
for _ in range(1000):
     env.render()
     env.step(env.action_space.sample()) # take a random action env.close()


import gym
import matplotlib.pyplot as plt

env = gym.make('CartPole-v1', render_mode='rgb_array')  # Use 'rgb_array' for Jupyter
obs = env.reset()

frames = []
for _ in range(100):
    frames.append(env.render())  # Collect frames
    action = env.action_space.sample()
    env.step(action)

env.close()

# Display one frame
plt.imshow(frames[0])
plt.axis('off')
plt.show()



import numpy as np

# Assuming you have defined the number of states and actions

num_states = 10  # Example number of states

num_actions = 5  # Example number of actions

# Initialize Q-table with zeros

Q_table = np.zeros((num_states, num_actions))

# Print initial Q-table

print("Initial Q-Table:")

print(Q_table)


pip install gym


import gym
import numpy as np
import random
from IPython.display import clear_output
import time

# Create the FrozenLake environment
env = gym.make('FrozenLake-v1', is_slippery=False)

# Initialize the Q-table
num_states = env.observation_space.n
num_actions = env.action_space.n
Q_table = np.zeros((num_states, num_actions))

# Parameters
total_episodes = 1000
learning_rate = 0.8
max_steps = 99
gamma = 0.95
epsilon = 1.0
max_epsilon = 1.0
min_epsilon = 0.01
decay_rate = 0.005

# Q-learning algorithm
for episode in range(total_episodes):
    state, _ = env.reset()  # Gym v26+ requires unpacking
    done = False

    for step in range(max_steps):
        # Choose action: Exploitation vs. Exploration
        if random.uniform(0, 1) > epsilon:
            action = np.argmax(Q_table[state, :])  # Exploit
        else:
            action = env.action_space.sample()  # Explore

        # Take action, observe outcome
        new_state, reward, done, _, _ = env.step(action)

        # Q-learning update rule
        Q_table[state, action] += learning_rate * (
            reward + gamma * np.max(Q_table[new_state, :]) - Q_table[state, action]
        )

        # Transition to new state
        state = new_state

        if done:
            break

    # Reduce epsilon for less exploration
    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)

    # Display progress every 100 episodes
    if episode % 100 == 0:
        clear_output(wait=True)
        print(f"Episode {episode}/{total_episodes} - Epsilon: {epsilon:.4f}")

# Print final Q-table
print("Final Q-table:")
print(Q_table)

# Close environment
env.close()




