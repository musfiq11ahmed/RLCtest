import gym
print(gym.__version__)


import gym
env = gym.make('CartPole-v0')
env.reset()
for _ in range(1000):
     env.render()
     env.step(env.action_space.sample()) # take a random action env.close()


import gym
import matplotlib.pyplot as plt

env = gym.make('CartPole-v1', render_mode='rgb_array')  # Use 'rgb_array' for Jupyter
obs = env.reset()

frames = []
for _ in range(100):
    frames.append(env.render())  # Collect frames
    action = env.action_space.sample()
    env.step(action)

env.close()

# Display one frame
plt.imshow(frames[0])
plt.axis('off')
plt.show()



import numpy as np

# Assuming you have defined the number of states and actions

num_states = 10  # Example number of states

num_actions = 5  # Example number of actions

# Initialize Q-table with zeros

Q_table = np.zeros((num_states, num_actions))

# Print initial Q-table

print("Initial Q-Table:")

print(Q_table)


pip install gym


import gym
import numpy as np
import random
from IPython.display import clear_output
import time

# Create the FrozenLake environment
env = gym.make('FrozenLake-v1', is_slippery=False)

# Initialize the Q-table
num_states = env.observation_space.n
num_actions = env.action_space.n
Q_table = np.zeros((num_states, num_actions))

# Parameters
total_episodes = 1000
learning_rate = 0.8
max_steps = 99
gamma = 0.95
epsilon = 1.0
max_epsilon = 1.0
min_epsilon = 0.01
decay_rate = 0.005

# Q-learning algorithm
for episode in range(total_episodes):
    state, _ = env.reset()  # Gym v26+ requires unpacking
    done = False

    for step in range(max_steps):
        # Choose action: Exploitation vs. Exploration
        if random.uniform(0, 1) > epsilon:
            action = np.argmax(Q_table[state, :])  # Exploit
        else:
            action = env.action_space.sample()  # Explore

        # Take action, observe outcome
        new_state, reward, done, _, _ = env.step(action)

        # Q-learning update rule
        Q_table[state, action] += learning_rate * (
            reward + gamma * np.max(Q_table[new_state, :]) - Q_table[state, action]
        )

        # Transition to new state
        state = new_state

        if done:
            break

    # Reduce epsilon for less exploration
    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)

    # Display progress every 100 episodes
    if episode % 100 == 0:
        clear_output(wait=True)
        print(f"Episode {episode}/{total_episodes} - Epsilon: {epsilon:.4f}")

# Print final Q-table
print("Final Q-table:")
print(Q_table)

# Close environment
env.close()



# Test the trained Q-learning agent
num_test_episodes = 5

for episode in range(num_test_episodes):
    state, _ = env.reset()
    done = False
    step = 0

    print(f"\nTest Episode {episode+1}")
    time.sleep(1)

    for step in range(max_steps):
        clear_output(wait=True)
        print(f"Step {step+1}")

        # Choose the best action
        action = np.argmax(Q_table[state, :])
        new_state, reward, done, _, _ = env.step(action)

        env.render()  # Render environment (only prints in CLI)

        if done:
            if reward == 1:
                print("Goal reached! üéâ")
            else:
                print("Fell in a hole. ‚ùå")
            time.sleep(2)
            break

        state = new_state
        time.sleep(0.5)

env.close()



import numpy as np
import random
import time

# Define the GridWorld environment
class GridWorld:
    def __init__(self):
        self.grid = np.array([
            [0, 0, 0, 1],  # Goal at (0, 3)
            [0, -1, 0, 0],  # Wall with reward -1
            [0, 0, 0, 0],
            [0, 0, 0, 0]  # Start at (3, 0)
        ])
        self.start_state = (3, 0)
        self.state = self.start_state

    def reset(self):
        self.state = self.start_state
        return self.state

    def is_terminal(self, state):
        return self.grid[state] == 1 or self.grid[state] == -1

    def get_next_state(self, state, action):
        next_state = list(state)
        if action == 0:  # Move up
            next_state[0] = max(0, state[0] - 1)
        elif action == 1:  # Move right
            next_state[1] = min(3, state[1] + 1)
        elif action == 2:  # Move down
            next_state[0] = min(3, state[0] + 1)
        elif action == 3:  # Move left
            next_state[1] = max(0, state[1] - 1)
        return tuple(next_state)

    def step(self, action):
        next_state = self.get_next_state(self.state, action)
        reward = self.grid[next_state]
        self.state = next_state
        done = self.is_terminal(next_state)
        return next_state, reward, done


# Define the Q-learning Agent
class QLearningAgent:
    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):
        self.q_table = np.zeros((4, 4, 4))  # Q-values for each state-action pair
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate

    def choose_action(self, state):
        if random.uniform(0, 1) < self.exploration_rate:
            return random.randint(0, 3)  # Explore (random action)
        else:
            return np.argmax(self.q_table[state])  # Exploit (best known action)

    def update_q_value(self, state, action, reward, next_state):
        max_future_q = np.max(self.q_table[next_state])  # Best Q-value for next state
        current_q = self.q_table[state][action]
        # Q-learning update rule
        self.q_table[state][action] = current_q + self.learning_rate * (
            reward + self.discount_factor * max_future_q - current_q
        )


# Train the agent using Q-learning
env = GridWorld()
agent = QLearningAgent()

episodes = 1000  # Number of training episodes

for episode in range(episodes):
    state = env.reset()  # Reset environment
    done = False

    while not done:
        action = agent.choose_action(state)  # Choose action
        next_state, reward, done = env.step(action)  # Take action
        agent.update_q_value(state, action, reward, next_state)  # Update Q-table
        state = next_state  # Move to next state


# Test the learned policy
state = env.reset()
done = False
print("\nLearned Policy Navigation:")

while not done:
    action = agent.choose_action(state)
    print(f"State: {state} -> Action: {action}")
    next_state, reward, done = env.step(action)
    state = next_state
    time.sleep(0.5)  # Pause for visualization

print("Reached Terminal State!")



!pip install pyglet==1.5.1 pyopengl pyvirtualdisplay
!apt update
!apt install -y xvfb ffmpeg



import pyglet

window = pyglet.window.Window(width=400, height=300, caption="Test Window")

@window.event
def on_draw():
    window.clear()

pyglet.app.run()



import gym

env = gym.make("CartPole-v1", render_mode="rgb_array")
env.reset()
env.render()

input("Press Enter to close the window...")  # Keeps the window open
env.close()



pip install gym[classic_control]



